{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "103f3ff1",
   "metadata": {},
   "source": [
    "<img src=\"images/housing_market.jpg_fit=scale\" width=400 height=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cafc43c",
   "metadata": {},
   "source": [
    "# King County, WA Housing Sales Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54561808",
   "metadata": {},
   "source": [
    "# Overview\n",
    "---\n",
    "\n",
    "Our team was hired by a hot-shot real estate agency to create a model which can predict the price of a house in the King County area based on differnt features. Our analysis concluded that the best features for prediction of housing prices are the following: city/town/neighborhood location, square footage, number of bedrooms, whether it's on the water, how good of a view it has, the property condition, and the age of the property. \n",
    "\n",
    "To acheive our goal, we used multiple linear regression models to analyse housing sales in King County, WA using housing data gathered within the county from 2014 and 2015. For this analysis, we used Ordinary Least Squared, Train-Test Split, and K-Fold Cross Validation regression models to create an efficient predictive model. \n",
    "\n",
    "Our analysis shows that based on the R^2 scores, our regression model's accuracy is 80.1% with 95% conf interval +-3.9% "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8659f784",
   "metadata": {},
   "source": [
    "# Business Understanding\n",
    "---\n",
    "Our stakeholder wants to be able to predict the price of a house based on certain features provided by their customers. \n",
    "\n",
    "Our project will answer the following questions:\n",
    "* With what accuracy could we predict the price of a house based on these features?\n",
    "* What specific housing features will provide us with the most accurate model? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85269af8",
   "metadata": {},
   "source": [
    "# Data Understanding\n",
    "---\n",
    "The data used for this project was sourced from a dataset called ‘King County House Sales’ and contains information regarding housing sales statistics in King County, WA.\n",
    "\n",
    "##### The dataset contains the following columns:\n",
    "\n",
    "* ```id```: A unique sale id relating to a house sale\n",
    "* ```date```: Date of house sale\n",
    "* ```price```: The price which the house sold for\n",
    "* ```bedrooms```: How many bedrooms the house has\n",
    "* ```bathrooms```: How many bathrooms the house has\n",
    "* ```sqft_living```: How much square footage the house has\n",
    "* ```sqft_lot```: How much square footage the lot has\n",
    "* ```floors```: How many floors the house has\n",
    "* ```waterfront```: Whether the house is on the waterfront. Originally contained ‘YES’ or ‘NO’, converted to 0 or 1 for comparative purposes\n",
    "* ```view```: Whether the house has a view and whether it’s fair, average, good, or excellent. Converted to numberical (0-4) for comparative purposes\n",
    "* ```condition```: overall condition of the house: Poor, Fair, Average, Good, Very Good\n",
    "* ```grade```: Numerical grading for house\n",
    "* ```sqft_above```: How much of the houses square footage is above ground\n",
    "* ```sqft_basement```: How much of the square footage is in the basement\n",
    "* ```yr_built```: Year the house was built\n",
    "* ```yr_renovated```: Year the house was renovated, if applicable\n",
    "* ```zipcode```: House zipcode\n",
    "* ```lat```: House’s latitude coordinate\n",
    "* ```long```: House’s longitude coordinate\n",
    "* ```sqft_living15```: Average size of living space for the closest 15 houses\n",
    "* ```sqft_lot15```: Average size of lot for the closest 15 houses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176f6265",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e064319c",
   "metadata": {},
   "source": [
    "### Importing required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d352b4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulations\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import statsmodels.api as sm\n",
    "from itertools import combinations\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "\n",
    "#SKlearn\n",
    "from sklearn.preprocessing import OneHotEncoder, PolynomialFeatures, StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import cross_validate, cross_val_predict\n",
    "\n",
    "\n",
    "#Visualisations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from warnings import simplefilter\n",
    "simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e8f14d",
   "metadata": {},
   "source": [
    "### Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4220ef7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/kc_house_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf34d20",
   "metadata": {},
   "source": [
    "### Processing cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f769e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore our data\n",
    "display(df.head())\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a974ec28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets convert condition codes to numerical values for the prediction model\n",
    "# Create function for conditional labeling:\n",
    "def condition_coding (condition):\n",
    "    \"\"\"\n",
    "    This will take the condition from string format and transform it into a corresponding code in integer format\n",
    "    Poor = 1, Fair = 2, Average = 3, Good = 4, Very Good = 5\n",
    "    \"\"\"\n",
    "    if condition == 'Poor':\n",
    "        condition_code = 1\n",
    "    elif condition == 'Fair':\n",
    "        condition_code = 2\n",
    "    elif condition == 'Average':\n",
    "        condition_code = 3\n",
    "    elif condition == 'Good':\n",
    "        condition_code = 4\n",
    "    elif condition == 'Very Good':\n",
    "        condition_code = 5\n",
    "    return condition_code\n",
    "\n",
    "# Apply function to dataframe\n",
    "df[\"condition_code\"] = df['condition'].map(condition_coding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e49054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets convert grade codes to numerical values for the prediction model\n",
    "# Create function for conditional labeling:\n",
    "def grade_coding (grade):\n",
    "    \"\"\"\n",
    "    This takes the grade in string format, splits it into a list of characters\n",
    "    It then concatenates the first two characters from the list and strips the whitespace and turns the result into an integer\n",
    "    We are left with a one or two digit integer correspondinng to the grade of the property\n",
    "    \"\"\"\n",
    "    grade_list = list(grade)\n",
    "    grade_code = int((grade_list[0] + grade_list[1]).strip())\n",
    "    return grade_code\n",
    "\n",
    "df['grade_code'] = df['grade'].map(grade_coding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c02d883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resolve missing values of house renovation dates. The column mode is 0, we fill missing values with it.\n",
    "df['yr_renovated'] = df['yr_renovated'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590b1cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create house age feature.\n",
    "# House age feature calculated the timeframe between transaction date and house construction date.\n",
    "df['age'] = df['date'].map(lambda x: int(x[-4:])) - df['yr_built']\n",
    "df.loc[df['age'] < 0, 'age'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcda6ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature was house renovated or not. \n",
    "def renovated (year):\n",
    "    \"\"\"\n",
    "    This returns a True / False value on whether a property has been renovated or not\n",
    "    \"\"\"\n",
    "    if year == 0.0:\n",
    "        return 0\n",
    "    elif year > 0.0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "df['renovated'] = df['yr_renovated'].map(renovated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832433ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This creates a column of float values stating how old the renovations are on a property\n",
    "# If a property has not been renovated it will have a 0.0 value\n",
    "df['age_of_renovations'] = df['date'].map(lambda x: int(x[-4:])) - df['yr_renovated']\n",
    "df.loc[df['age_of_renovations'] == 2014, 'age_of_renovations'] = 1\n",
    "df.loc[df['age_of_renovations'] == 2015, 'age_of_renovations'] = 1\n",
    "df['age_of_renovations'] = df['age_of_renovations'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4a6b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The majority of values in waterfront column are - No. Fill the missing values with mode value if this column.\n",
    "df['waterfront'] = df['waterfront'].fillna('NO')\n",
    "\n",
    "# Convert waterfront column to numerical values for the prediction model \n",
    "df['waterfront_coded'] = df['waterfront'].map({'NO':0, 'YES':1})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719bcf66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The majority of values in view column are - None. Fill the missing values with mode value if this column.\n",
    "df['view'] = df['view'].fillna('NONE')\n",
    "\n",
    "# Label view column for the prediction model.\n",
    "df['view_coded'] = df['view'].map({'NONE':0, 'FAIR':1, 'AVERAGE':2, 'GOOD':3, 'EXCELLENT':4})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4244f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some values of sqft_basementa are missing. Find the missing values by subtracting sqft_above from sqft living.\n",
    "df['sqft_basement'] = np.where(df['sqft_basement'] == '?', df['sqft_living'] - df['sqft_above'], df['sqft_basement'])\n",
    "df['sqft_basement'] = df['sqft_basement'].astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7043c6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shape of finall dataframe without geo location data\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a052a54a",
   "metadata": {},
   "source": [
    "### Geo Locate to convert lat/long columns into Cities/Towns/Neighborhoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb7a0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# location coordinates were scrapped from OSM in other jupyter notebook. \n",
    "# Load processed geo data from another notebook\n",
    "with open('./data/Data_frame_geoloc.pickle', 'rb') as df_geo_data:\n",
    "    df_geo = pickle.load(df_geo_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd5185e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets check size of dataframe, drop all dublicated columns.\n",
    "df_geo.shape\n",
    "df_geo.drop(['id', 'lat', 'price', 'yr_built', 'sqft_living', 'sqft_lot'], axis = 1 , inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a581ba8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_geo.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9134314b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge 2 dataframes and check all columns.\n",
    "df = pd.concat([df,df_geo], axis =1, verify_integrity = True )\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca2bfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate states and counties for noise.\n",
    "print(df[\"state\"].value_counts()) #all recods from Washington state\n",
    "print(df[\"county\"].value_counts()) # 4 records from different counties. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97944fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove records from other counties\n",
    "df.drop(df[df.county != \"King County\"].index, inplace = True)\n",
    "df.shape\n",
    "#Drop county and state columns to reduce the number of features\n",
    "df.drop(\"state\", axis = 1, inplace = True)\n",
    "df.drop(\"county\", axis = 1, inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b145731",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73dc4f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate \"city\" column for missing values\n",
    "print(f\"The number of records with missing cities {sum(df.city.isna())}\") #779 records have no cities in it. \n",
    "print(f\"Missing values are in {round(sum(df.city.isna())/df.shape[0]*100,2)} % of data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9980466",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove records with missing cities\n",
    "df.drop(df[df.city.isna() == True].index, axis = 0, inplace = True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcd6409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate \"Type_place\" column for missing values\n",
    "print(f\"The number of records with missing values {sum(df.Type_place.isna())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c617aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate \"suburb\" column for missing values\n",
    "print(f\"The number of records with missing values {sum(df.suburb.isna())}\")\n",
    "print(f\"Missing \\\"suburb\\\"values are in {round(sum(df.suburb.isna())/df.shape[0]*100,2)} % of data\")\n",
    "df_geo.suburb.value_counts() #There is no strong patterns in this data\n",
    "print(\"Missing \\\"suburb\\\" values for cities are in \", round(sum(df[df.Type_place == \"city\"].suburb.value_counts())/df[df.Type_place == \"city\"].shape[0]*100,2), \"% of records\")\n",
    "# We won't proceed with this data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e1058b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outliers in bedrooms\n",
    "df.drop(df[(df.bedrooms == 33)].index, axis = 0, inplace = True)\n",
    "df.drop(df[(df.bedrooms == 11)].index, axis = 0, inplace = True)\n",
    "df.drop(df[(df.bedrooms == 10)].index, axis = 0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7999202f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all cathegorical values that were coverted to numerical\n",
    "to_drop_cat = [\"date\", \"waterfront\", \"view\", \"condition\", \"grade\"]\n",
    "df.drop(to_drop_cat, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65e687f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reset indexes\n",
    "df_fin = df.reset_index().drop(\"index\", axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac7eaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different type of geo columns\n",
    "Geo_columns_basic_all = [\"Type_place\", \"city\"]\n",
    "Geo_columns_basic_type = [\"Type_place\"]\n",
    "Geo_columns_basic_city_names = [\"city\"]\n",
    "Geo_columns_advanced = [\"suburb\"]\n",
    "Geo_columns_drop = [\"To_drop_place_ID\", \"To_drop_road\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13bcd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the type of geo columns that will be used for modelling\n",
    "# Model one : encode each city into our dataframe\n",
    "modeling_columns = Geo_columns_basic_city_names\n",
    "Geo_columns_drop = [column for column in list(df_fin.columns) if ((column not in modeling_columns)  and (column != \"id\"))]\n",
    "\n",
    "#seting up data categorical data for encoding !!!!!! (need to change later on)\n",
    "X_cat_geo = df_fin.drop(Geo_columns_drop, axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c810a8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup One Hot Encoder \n",
    "encoder_geo_basic = OneHotEncoder(handle_unknown = \"ignore\")\n",
    "fit_df = X_cat_geo.drop(\"id\", axis = 1)\n",
    "encoder_geo_basic.fit(fit_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542abebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare transformed dataset\n",
    "X_cat_transf=encoder_geo_basic.transform(fit_df)\n",
    "X_geo_df = pd.DataFrame(X_cat_transf.todense(), columns = encoder_geo_basic.get_feature_names())\n",
    "#X_geo_df[\"id\"] = X_cat_geo[\"id\"]\n",
    "df_cities_columns = encoder_geo_basic.get_feature_names() # New added column names. If you need them later on.\n",
    "df_cities = pd.concat([df_fin, X_geo_df], axis = 1)       # DF with coded cities/towns/villages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f3073d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the type of geo columns that will be used for modelling\n",
    "# Model two : encode only size of city into model\n",
    "modeling_columns = Geo_columns_basic_type\n",
    "Geo_columns_drop = [column for column in list(df_fin.columns) if ((column not in modeling_columns)  and (column != \"id\"))]\n",
    "\n",
    "#seting up data categorical data for encoding !!!!!! (need to change later on)\n",
    "X_cat_geo = df_fin.drop(Geo_columns_drop, axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d787d3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup One Hot Encoder \n",
    "encoder_geo_basic = OneHotEncoder(handle_unknown = \"ignore\")\n",
    "fit_df = X_cat_geo.drop(\"id\", axis = 1)\n",
    "encoder_geo_basic.fit(fit_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e7e9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare transformed dataset\n",
    "X_cat_transf=encoder_geo_basic.transform(fit_df)\n",
    "X_geo_df = pd.DataFrame(X_cat_transf.todense(), columns = encoder_geo_basic.get_feature_names())\n",
    "#X_geo_df[\"id\"] = X_cat_geo[\"id\"]\n",
    "df_types_columns = encoder_geo_basic.get_feature_names() # New added column names. If you need them later on.\n",
    "df_types = pd.concat([df_fin, X_geo_df], axis = 1)       # DF with coded cities/towns/villages\n",
    "df_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1a7be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cities.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d785707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drob geo cathegorical columns that were transfered to numerical.\n",
    "to_drop_index = [(number) for number, column in enumerate(df_cities.dtypes) if column == 'O']\n",
    "to_drop_columns = list(df_cities.columns[to_drop_index])\n",
    "df_cities = df_cities.drop(to_drop_columns, axis =1)\n",
    "\n",
    "to_drop_index = [(number) for number, column in enumerate(df_types.dtypes) if column == 'O']\n",
    "to_drop_columns = list(df_types.columns[to_drop_index])\n",
    "df_types = df_types.drop(to_drop_columns, axis =1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306a48d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final clean_up\n",
    "final_drop = [\"zipcode\", \"id\", \"zipcode\", \"lat\", \"long\", \"lon\",\"To_drop_place_ID\"]\n",
    "df_cities = df_cities.drop(final_drop, axis = 1)\n",
    "df_types = df_types.drop(final_drop, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af5d7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output from data cleaning: 2 dataframes, cleaned from noise and with corrected data.\n",
    "# df_cities - dataframe with encoded cities where property is located\n",
    "# df_types - dataframe with encoded type of location (city, town or village)\n",
    "\n",
    "df_cities\n",
    "df_types\n",
    "df_cities.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cb2130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_cities is our final merged df which we will base our modeling off of\n",
    "df_cities.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16d35f7",
   "metadata": {},
   "source": [
    "# Data Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ace0c50",
   "metadata": {},
   "source": [
    "## Analysing Regression with Property Feature Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff1f3cb",
   "metadata": {},
   "source": [
    "We want to look data corresponding to the property features: square footage, number of bedrooms, number of bathrooms, view quality, and waterfront location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3094f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a correlation heat map for all the columns relating to house features (sqft, bedrooms, floors, etc)\n",
    "# checks if there are any features that might cause collinearity issues\n",
    "\n",
    "cols = ['price', 'bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors', 'waterfront_coded', 'view_coded', 'sqft_above', 'sqft_basement', 'sqft_living15', 'sqft_lot15']\n",
    "corr = df_cities[cols].corr()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,10)) \n",
    "sns.heatmap(corr, annot=True, ax=ax);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35807408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize and numerical columns that dont contain ordinal or categorical numbers\n",
    "col_norm = ['sqft_living', 'sqft_lot', 'sqft_above', 'sqft_basement', 'sqft_living15', 'sqft_lot15']\n",
    "\n",
    "def norm(series):\n",
    "    return (series - series.mean())/series.std()\n",
    "for feat in col_norm:\n",
    "    df_cities[feat+\"_norm\"] = norm(df_cities[feat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e899e4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function creates an ordinary least squares model for price using input columns and data frame \n",
    "def ols_model(cols, df):\n",
    "    predictors = '+'.join(cols)\n",
    "    formula = 'price' + '~' + predictors\n",
    "    model = ols(formula=formula, data=df).fit()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b34eba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using an ols model to look at p-values and R^2 values for all of the feature columns\n",
    "col_pred = ['bedrooms', 'bathrooms', 'sqft_living_norm', 'sqft_lot_norm', 'floors', 'waterfront_coded', 'view_coded', 'sqft_above_norm', 'sqft_basement_norm', 'sqft_living15_norm', 'sqft_lot15_norm']\n",
    "\n",
    "ols_model(col_pred, df_cities).summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16774445",
   "metadata": {},
   "source": [
    "### Multicollinearity Issues\n",
    "---\n",
    "\n",
    "There are potential multicollinearity issues with **sqft_above** and **sqft_living**, **sqft_living** and **bathrooms**, **sqft_living15** and **sqft_living**, and **sqft_lot15** and **sqft_lot** - this is due to high correlation values\n",
    "\n",
    "**bathrooms** and **sqft_basement** have proven to not be statistically significant (p > 0.05)\n",
    "\n",
    "Best option to remove **sqft_above**, **sqft_basement**, **sqft_living15**, **sqft_lot15**, and **bathrooms** columns to avoid any potential modeling issues\n",
    "\n",
    "**sqft_lot** and **floors** did not have a significant effect on the R^2 value for the ols model, so we have removed these column from the model as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3128236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finalized list of features to be used for regression\n",
    "cols_features = ['bedrooms', 'sqft_living_norm', 'waterfront_coded', 'view_coded']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad7c213",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# below is the ols model for the final selection of columns for house features\n",
    "# chose these columns based on p-value and how each effected R^2 value\n",
    "\n",
    "ols_model(cols_features, df_cities).summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0187ba7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the unused feature columns from the data frame to continue with out regression modeling\n",
    "drop_cols = ['sqft_lot', 'sqft_above', 'sqft_basement', 'bathrooms', 'floors', 'sqft_living', 'sqft_living15', 'sqft_lot15', 'sqft_lot15_norm', \\\n",
    "            'sqft_living15_norm', 'sqft_lot_norm', 'sqft_above_norm', 'sqft_basement_norm']\n",
    "df_cities = df_cities.drop(drop_cols, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56277f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cities.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8213689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a train test split model to output the R^2 value and the mean squared error for the house feature columns\n",
    "X = df_cities[cols_features]\n",
    "y = df_cities['price']\n",
    "\n",
    "#create train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "#create model\n",
    "poly = PolynomialFeatures(2)\n",
    "X_poly = poly.fit_transform(X_train)\n",
    "model = LinearRegression()\n",
    "model.fit(X_poly, y_train)\n",
    "\n",
    "#predict model\n",
    "predict_train = model.predict(X_poly)\n",
    "predict_test = model.predict(poly.transform(X_test))\n",
    "mse = mean_squared_error(y_test, predict_test)\n",
    "\n",
    "#score model\n",
    "train_score = model.score(X_poly, y_train)\n",
    "test_score = model.score(poly.transform(X_test), y_test)\n",
    "\n",
    "train_score, test_score, mse\n",
    "\n",
    "\n",
    "print(f'Train Score: {train_score}\\nTest Score: {test_score}\\nMean Squared Error: {mse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050fda9b",
   "metadata": {},
   "source": [
    "## Analysing Regression With Property Condition Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76940c9",
   "metadata": {},
   "source": [
    "Looking at the data corresponding to the condition of the property we subset the following columns:\n",
    "'condition_code', 'grade_code', 'age', 'age_of_renovations', and 'renovated'\n",
    "\n",
    "We then look at correlations between these variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03ed451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First I'll crate some basic variables that will be use through the analysis and modeling\n",
    "df_condition = df_cities[['price','condition_code','grade_code','age','age_of_renovations','renovated']]\n",
    "X_condition = df_condition[['condition_code','grade_code','age','age_of_renovations','renovated']]\n",
    "y_condition = df_condition.price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cbae97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lest look at the feature correlations\n",
    "X_condition.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed38486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can view this as a heat map to give a better view\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(X_condition.corr(), annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdab9a5",
   "metadata": {},
   "source": [
    "We can see that 'age_of_renovations' is closely correlated with 'renovated' and may cause multicolinearity issues. Indeed they are related to the same characteristsic and tell the same tale. We can eliminate one of these from our analysis.\n",
    "\n",
    "If a property is showing a value > 1 in 'age_of_renovations' then we know it has been renovated which tells us the same as the 'renovated' column... so we will drop 'renovated'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12a2076",
   "metadata": {},
   "outputs": [],
   "source": [
    "formula_cond = 'price ~ condition_code + grade_code + age + age_of_renovations'\n",
    "\n",
    "condition_model = ols(formula=formula_cond, data=df_condition).fit()\n",
    "condition_model_summ = condition_model.summary()\n",
    "\n",
    "print(condition_model_summ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2081f4f5",
   "metadata": {},
   "source": [
    "Lets take a look at some regressions and see what will give us the strongest model based on the condition variables\n",
    "\n",
    "Using all of our features, we get a strong score on both a training data set and also the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845a2713",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_condition, \n",
    "                                                    y_condition,\n",
    "                                                    test_size=None,\n",
    "                                                    random_state=42\n",
    "                                                   )\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "lr.score(X_train, y_train)\n",
    "lr.score(X_test, y_test)\n",
    "\n",
    "print(lr.score(X_train, y_train))\n",
    "print(lr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654cc966",
   "metadata": {},
   "source": [
    "We don't need all of these features though so lets iterate through some linear regressions with different feature combinations and see what will give us the simplest, but highest scoring model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae7973f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping the 'renovated' feature that was giving us multicolinearity issues\n",
    "\n",
    "X_condition_1 = df_condition[['condition_code','grade_code','age','age_of_renovations']]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_condition_1, \n",
    "                                                    y_condition,\n",
    "                                                    test_size=None,\n",
    "                                                    random_state=42\n",
    "                                                   )\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "print(lr.score(X_train, y_train))\n",
    "print(lr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ad2818",
   "metadata": {},
   "source": [
    "Conditon and Grade tell a similar story about a property. Looking at he data we see Grade is a lot more in depth and provides more bins, so it is potentially more powerful. Lets drop Condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e052b3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping the 'condition_code' feature from the model\n",
    "\n",
    "X_condition_2 = df_condition[['grade_code','age','age_of_renovations']]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_condition_2, \n",
    "                                                    y_condition,\n",
    "                                                    test_size=None,\n",
    "                                                    random_state=42\n",
    "                                                   )\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "print(lr.score(X_train, y_train))\n",
    "print(lr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa812f26",
   "metadata": {},
   "source": [
    "That barely chaged our model score, and indeed when we look at just Condition it doesn't score well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb5f664",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_condition_3 = df_condition[['condition_code']]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_condition_3, \n",
    "                                                    y_condition,\n",
    "                                                    test_size=None,\n",
    "                                                    random_state=42\n",
    "                                                   )\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "print(lr.score(X_train, y_train))\n",
    "print(lr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f2638e",
   "metadata": {},
   "source": [
    "However, looking at just Grade we can see that it scores well and is indeed our strongest feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72712582",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_condition_4 = df_condition[['grade_code']]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_condition_4, \n",
    "                                                    y_condition,\n",
    "                                                    test_size=None,\n",
    "                                                    random_state=42\n",
    "                                                   )\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "print(lr.score(X_train, y_train))\n",
    "print(lr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df06ccfd",
   "metadata": {},
   "source": [
    "Including the 'age_of_renovations' feature does not add much to the model, so we will drop that as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9bfa3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_condition_5 = df_condition[['grade_code','age_of_renovations']]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_condition_5, \n",
    "                                                    y_condition,\n",
    "                                                    test_size=None,\n",
    "                                                    random_state=42\n",
    "                                                   )\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "print(lr.score(X_train, y_train))\n",
    "print(lr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda1ed14",
   "metadata": {},
   "source": [
    "*Interestingly, Age does not score well by itself. However, when coupled with the Grade feature it gives a large improvement to the overall score.\n",
    "\n",
    "This combination of features gives us the highest score, with the least features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0fe23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_condition_6 = df_condition[['age']]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_condition_6, \n",
    "                                                    y_condition,\n",
    "                                                    test_size=None,\n",
    "                                                    random_state=42\n",
    "                                                   )\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "print(lr.score(X_train, y_train))\n",
    "print(lr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad61172",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_condition_7 = df_condition[['grade_code','age']]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_condition_7, \n",
    "                                                    y_condition,\n",
    "                                                    test_size=None,\n",
    "                                                    random_state=42\n",
    "                                                   )\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "print(lr.score(X_train, y_train))\n",
    "print(lr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc60a5b",
   "metadata": {},
   "source": [
    "Indeed, these are the two columns we will include on the overall \"Linear Regression\" model\n",
    "\n",
    "Lets now look at some Polynomial regressions on the \"condition\" data and see if we can improve on our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c1b05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_conditions = [X_condition, X_condition_1, X_condition_2, X_condition_3, X_condition_4, X_condition_5, X_condition_6, X_condition_7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3ebc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function thet interates through our combinations to determine the the best number of Polynomial Features\n",
    "# It returns an array of train and test scores for each combination\n",
    "\n",
    "def poly_scores_array (X_lst, y):\n",
    "    poly_scores = []\n",
    "    for X in X_lst:\n",
    "        train_scores = []\n",
    "        test_scores = []\n",
    "        for i in range(1,7):\n",
    "            poly = PolynomialFeatures(i)\n",
    "            X_poly = pd.DataFrame(poly.fit_transform(X))\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X_poly, y, test_size=None, random_state=42)\n",
    "            model = LinearRegression()\n",
    "            model.fit(X_train, y_train)\n",
    "            train_scores.append(model.score(X_train, y_train))\n",
    "            test_scores.append(model.score(X_test, y_test))\n",
    "            scores = (train_scores, test_scores)\n",
    "            poly_scores.append(scores)\n",
    "    return poly_scores\n",
    "\n",
    "#uncomment code to see output array\n",
    "#poly_scores_array(X_conditions, y_condition)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1306ebfb",
   "metadata": {},
   "source": [
    "Just eyeballing the array we conistently get our highest scores up to 4 polynomial features and then our test scores drop significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09057860",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Having determined the best number of polynomial features is 4 for the property condition data\n",
    "# Create a function to iterate through our feature combinations to determine the best model\n",
    "\n",
    "def best_poly_model (X_lst, y):\n",
    "    poly_model_scores = []\n",
    "    for X in X_lst:\n",
    "        poly_2 = PolynomialFeatures(4)\n",
    "        X_poly = pd.DataFrame(poly_2.fit_transform(X))\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_poly, y,\n",
    "                                                    test_size=None,\n",
    "                                                    random_state=42)\n",
    "        lr_poly = LinearRegression()\n",
    "        lr_poly.fit(X_train, y_train)\n",
    "        train_score = lr_poly.score(X_train, y_train)\n",
    "        test_score = lr_poly.score(X_test, y_test)\n",
    "        score = (train_score, test_score)\n",
    "        poly_model_scores.append(score)\n",
    "    return poly_model_scores\n",
    "\n",
    "best_poly_model (X_conditions, y_condition)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943e940a",
   "metadata": {},
   "source": [
    "The array above corresponds to our feature combinations. In our linear regression analysis we determined that X_condition_7 gave us our best model and we can see above that the same combination gives us excellent results and even scores the best on the test data overall.\n",
    "\n",
    "Lets see this isolated below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedbd70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_7 = PolynomialFeatures(4)\n",
    "X_poly_7 = pd.DataFrame(poly_7.fit_transform(X_condition_7))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_poly_7, y_condition,\n",
    "                                                    test_size=None,\n",
    "                                                    random_state=42)\n",
    "lr_poly_7 = LinearRegression()\n",
    "lr_poly_7.fit(X_train, y_train)\n",
    "\n",
    "print(lr_poly_7.score(X_train, y_train))\n",
    "print(lr_poly_7.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4be927b",
   "metadata": {},
   "source": [
    "After performing both linear and ploynomial regression analysis on the Property Condition data, we have determined that _Grade (as coded in 'gradecode') and Age are the best variables to use in our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312bbda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = ['yr_built', 'yr_renovated', 'condition_code', 'renovated', 'age_of_renovations']\n",
    "df_cities = df_cities.drop(cols_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a86c7c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_cities.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e4cf7c",
   "metadata": {},
   "source": [
    "### Regression Model for Property Feature Columns and Property Condition Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183a395e",
   "metadata": {},
   "source": [
    "Now that we have narrowed down our property feature columns and property condition columns we can create a new regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bceb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_feat_cond = ['bedrooms', 'sqft_living_norm', 'waterfront_coded', 'view_coded', 'grade_code', 'age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1667d808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a train test split model to output the R^2 value and the mean squared error \n",
    "# for the property feature columns combined with property condition columns\n",
    "X = df_cities[cols_feat_cond]\n",
    "y = df_cities['price']\n",
    "\n",
    "#create train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "#create model - polynomial 2 is the best fit for this data (highest scores and lowest mean squared error)\n",
    "poly = PolynomialFeatures(2)\n",
    "X_poly = poly.fit_transform(X_train)\n",
    "model = LinearRegression()\n",
    "model.fit(X_poly, y_train)\n",
    "\n",
    "#predict model\n",
    "predict_train = model.predict(X_poly)\n",
    "predict_test = model.predict(poly.transform(X_test))\n",
    "mse = mean_squared_error(y_test, predict_test)\n",
    "\n",
    "#score model\n",
    "train_score = model.score(X_poly, y_train)\n",
    "test_score = model.score(poly.transform(X_test), y_test)\n",
    "\n",
    "train_score, test_score, mse\n",
    "\n",
    "\n",
    "print(f'Train Score: {train_score}\\nTest Score: {test_score}\\nMean Squared Error: {mse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235790b2",
   "metadata": {},
   "source": [
    "As we can see the R^2 scores for our new model is better than both the Property Features Model and the Property Condition Model. This shows us that are new data better fits our regression line!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c52e7d",
   "metadata": {},
   "source": [
    "Now it's time to add in our location data for our final regression model!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1042522c",
   "metadata": {},
   "source": [
    "# Regression Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8eebe4",
   "metadata": {},
   "source": [
    "Below we have created a linear regression calculator which will output our R^2 value (model accuracy) as well as the mean squared error for our finalized dataset. It will also print out a random sample of our predictive model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57417a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# function for modeling linear regression \n",
    "from warnings import simplefilter\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "def linear_reg_calculator(Data, n = 1, to_drop = []):\n",
    "    #model preparation\n",
    "    Data = Data.reset_index().drop(\"index\", axis = 1)\n",
    "    poly = PolynomialFeatures(n)\n",
    "    linreg = LinearRegression()\n",
    "    scoring_model = [\"r2\", \"neg_mean_squared_error\"]\n",
    "    kf = KFold(n_splits=5)\n",
    "    y = Data[\"price\"]\n",
    "    X = Data.drop(\"price\", axis = 1)    \n",
    "    # execute code that will generate warnings\n",
    "    # Fit and transform X for polynomial function\n",
    "    if n > 1:\n",
    "        X_new = X.drop(df_cities_columns, axis = 1)    # Drop geo categorical data\n",
    "        if to_drop != []:\n",
    "            X_new2 = X_new.drop(to_drop, axis = 1)     # Drop any additional columns we want\n",
    "            X_new = X_new2  \n",
    "        else:\n",
    "            pass\n",
    "        X_poly= poly.fit_transform(X_new)                                        # Poly transformation on numerical data\n",
    "        X = pd.concat([pd.DataFrame(X_poly), Data[df_cities_columns]], axis = 1) # Making new dataframe\n",
    "        \n",
    "    \n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    # Modelling\n",
    "    reg_poly = cross_validate(linreg, X, y, scoring= scoring_model, cv = kf)\n",
    "    mean = np.mean(reg_poly[\"test_r2\"])\n",
    "    stand = np.std(reg_poly[\"test_r2\"])\n",
    "    mean_mean_sq_error = -np.mean(reg_poly[\"test_neg_mean_squared_error\"])\n",
    "    \n",
    "    # Random house experiment\n",
    "    difference_list=[]\n",
    "    random_sample = np.random.randint(0, len(y))                         # Choose the random house from our prediction\n",
    "    y_hat = cross_val_predict(linreg, X, y, cv = kf)                     # Make predictions based on our model \n",
    "    price = round(y[random_sample],0)                                    # sample house price\n",
    "    predicted_price = round(y_hat[random_sample], 0)                     # sample house predicted price\n",
    "    difference = round(100*(y_hat[random_sample]/y[random_sample]-1),2)  # difference between prices\n",
    "    \n",
    "    # Random 20 houses prediction\n",
    "    for i in list(range(20)):\n",
    "        random_sample = np.random.randint(0, len(y))\n",
    "        difference_list.append(round(100*(y_hat[random_sample]/y[random_sample]-1),2))\n",
    "    difference_mean = round(np.mean(difference_list),2)    \n",
    "    difference_std = round(np.std(difference_list),2)    \n",
    "    \n",
    "    #Outputs\n",
    "    print(f\"Model accuracy based on R2 score {round(mean*100,1)}% with 95% conf interval +-{round(2*stand*100,1)}%\")\n",
    "    print(f\"Mean Squared Error {round(mean_mean_sq_error, 0)}\")\n",
    "    print(f\"Random house price {price}, predicted price {predicted_price}, difference {difference} %\")\n",
    "    print(f\"Sample(20 houses) prediction price difference {difference_mean} %\")\n",
    "    return mean_mean_sq_error, mean, y_hat, y\n",
    "    \n",
    "#Work example\n",
    "#(MSE, mean_ac, y_hat_regr, y_reg) = linear_reg_calculator(df_cities, 2, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a14e4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "(MSE, mean_ac, y_hat_regr, y_reg) = linear_reg_calculator(df_cities, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b7066f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Linear regression performance plot.\n",
    "def currency(x, pos):\n",
    "    \"\"\"The two args are the value and tick position\"\"\"\n",
    "    if x >= 1e9:\n",
    "        s = '${:1.1f}B'.format(x*1e-9)\n",
    "    elif x >= 1e6:\n",
    "        s = '${:1.1f}M'.format(x*1e-6)\n",
    "    else:\n",
    "        s = '${:1.0f}K'.format(x*1e-3)\n",
    "    return s\n",
    "\n",
    "x_line = np.linspace(0,2000000)\n",
    "fig,axs = plt.subplots(figsize = (12,8))\n",
    "sns.scatterplot(y_reg,y_hat_regr , marker = \".\" , s = 8,alpha = 1, label = \"Individual house sell price\")\n",
    "axs.plot(x_line, x_line, color =\"red\", label = \"Ideal prediction line\")\n",
    "axs.set_xlim(0,2000000) ; axs.set_ylim(0,2000000)\n",
    "axs.yaxis.set_major_formatter(currency)\n",
    "axs.xaxis.set_major_formatter(currency)\n",
    "axs.set_aspect(\"equal\")\n",
    "axs.set_title(\"House Price vs house predicted price plot\")\n",
    "axs.set_xlabel(\"House sell price\")\n",
    "axs.set_ylabel(\"House predicted sell price\")\n",
    "axs.legend();\n",
    "plt.savefig(\"./images/PricevsPredict_scatter.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbea1a1e",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "---\n",
    "Our final linear regression model can conclude an accuracy of %80.1 based on the R^2 score. \n",
    "\n",
    "Our House Predicted Price scatter plot shows how well our predicted regression model fits the individual selling prices of houses\n",
    "\n",
    "This data tells us that with our selected feature columns (neighborhood, age, grade code, bedrooms, living square footage, waterfront, and view quality) we have created a model that can predict house prices based on features with an %80.1 accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
